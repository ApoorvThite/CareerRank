# Data Files

The following large data files are excluded from the repository due to GitHub file size limits:

## Required Data Files (Not in Repo)

Place these files in the project root directory:

1. **`profiles.csv`** (50 MB)
   - Individual profile information
   - 50,000 profiles with skills, experience, education, etc.

2. **`compatibility_pairs.csv`** (1.06 GB)
   - Pairwise compatibility scores
   - ~5 million profile pairs with compatibility metrics

## Generated Artifacts (Not in Repo)

These files are generated by running the preprocessing and training scripts:

### Preprocessing Outputs
- `artifacts/serialized_profiles.csv` - Text representations of profiles
- `artifacts/pair_text_dataset.csv` (5.2 GB) - Text pairs for training

### Model Files
- `artifacts/models/bi_encoder/model.safetensors` (87 MB) - Bi-encoder weights
- `artifacts/embeddings/profile_embeddings.npy` (73 MB) - Profile embeddings
- `artifacts/index/faiss_profiles.index` (73 MB) - FAISS search index

## How to Regenerate

1. **Preprocessing (Day 1)**
   ```bash
   python preprocessing/day1_eda_and_cleaning.py
   python preprocessing/create_splits.py
   python preprocessing/build_eval_sets.py
   ```

2. **Baselines (Day 2)**
   ```bash
   python baselines/tfidf_retrieval_improved.py
   python baselines/structured_regression_improved.py
   python baselines/hybrid_baseline.py
   python evaluation/baseline_eval_improved.py
   ```

3. **Bi-Encoder (Day 3)**
   ```bash
   python training/use_pretrained_bi_encoder.py
   python retrieval/build_faiss_index_simple.py
   python evaluation/eval_bi_encoder_simple.py
   ```

## Alternative: Download Pre-generated Files

If you have access to the original data source, download:
- Original data files: `profiles.csv`, `compatibility_pairs.csv`
- Place them in the project root
- Run the scripts above to regenerate all artifacts

## Storage Requirements

- Original data: ~1.1 GB
- Generated artifacts: ~5.4 GB
- Total: ~6.5 GB

**Note:** Consider using Git LFS or cloud storage (S3, Google Drive) for sharing large data files.
